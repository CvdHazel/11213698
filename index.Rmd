---
title: "Portfolio Computational Musicology"
author: "Chayenne van den Hazel"
date: "3 maart 2019"
output: 
    flexdashboard::flex_dashboard:
        storyboard: true
        theme: lumen
---

```{r}
library(tidyverse)
library(tidymodels)
library(ggdendro)
library(protoclust)
library(heatmaply)
library(spotifyr)
library(compmus)
source('spotify.R')
```

### What is this storyboard about? 

![Bring Me The Horizon](bmth2019.jpg)

***

The rock/metal band Bring Me The Horizon released their sixth album *amo* on the 25th of January 2019. The reaction on the new album is divided. The sound of this album is totally different than fans are used to hearing from them. It feels like they are stepping out of their 'heavy metal' image and taking on a new one. One song called 'heavy metal' of Bring Me The Horizon's new album talks about their change in genre and the reaction of their fans:

*"And I keep picking petals
I'm afraid you don't love me anymore
'Cause a kid on the 'gram in a Black Dahlia tank
Says it ain't heavy metal"*

I want to know if their new album is really that different from the other albums and what the difference is exactly. Is it true that they are changing their image and what are they changing it to? My research question is therefore: Is Bring Me The Horizon creating a new musical image and what is this image exactly?

To anwer my research question I'm using a handfull of different Spofity playlists to compare their features. My main corpus is obviously Bring Me The Horizon's newest album *amo*. Besides that I'm also looking into all their old albums: *Count Your Blessings*, *Suidice Season*, *There is a Hell Believe Me I've Seen it. There is a Heaven Let's Keep it a Secret*, *Sempiternal* and *That's the Spirit*. To get the Spotify features for the albums, I've make seperate playlist for each. With the help of these albums I can compare the new album to their old ones and see if there are differences and similarities. I'm not only doing this between albums, but also with genres. I'm using different Spotify playlists to compare them to the albums and see which genre fits the best per album. The playlist I'm using for this are: Rock Classics, Pure Pop Punk, Heavy Metal, Pop Internacional and Ultimate Indie. By using the Bring Me The Horizon albums and the genre playlist, I hope to answer my research question 


### Is Bring Me The Horizon's newest album really that different from the rest?

```{r}
# Data Spotify features 

blessings <- get_playlist_audio_features('faaske', '3q4J2ymyPxuRH49FUexRDA')
amo <- get_playlist_audio_features('faaske', '1GEk4D55A3EI7uKAcr1N0w')
spirit <- get_playlist_audio_features('faaske', '4cIxKAxJ55NmI5Cz5Gp1jn')
sempiternal <- get_playlist_audio_features('faaske', '0lNxfrMHxBMcUnQps2DkeU')
hell <- get_playlist_audio_features('faaske', '0SIOLbQcEkakGKEmClItXd')
suicide <- get_playlist_audio_features('faaske', '0RSC1KIQe8DOpLGdgKZLcE')

bmth <- get_artist_audio_features('Bring Me The Horizon')


# Combine data sets

albums <-
    blessings %>% mutate(album = "1. Count your blessings") %>%
    bind_rows(amo %>% mutate(album = "6. amo")) %>%
    bind_rows(spirit %>% mutate(album = "5. That's the Spirit")) %>%
    bind_rows(sempiternal %>% mutate(album = "4. Sempiternal")) %>%
    bind_rows(hell %>% mutate(album = "3. There is a Hell...")) %>%
    bind_rows(suicide %>% mutate(album = "2. Suicide Season"))

#Plot valence/energy first and last ablum 

ve_album <- albums %>%                       # Start with awards.
    ggplot(                      # Set up the plot.
        aes(
            x = valence,
            y = energy,
            size = loudness,
            colour = album,
            label = track_name 
        )
    ) +
    geom_point(alpha = 0.8) +               # Scatter plot.
    geom_rug(size = 0.1) +       # Add 'fringes' to show data distribution.
    scale_x_continuous(          # Fine-tune the x axis.
        limits = c(0, 1),
        breaks = c(0, 0.50, 1),  # Use grid-lines for quadrants only.
        minor_breaks = NULL      # Remove 'minor' grid-lines.
    ) +
    scale_y_continuous(          # Fine-tune the y axis in the same way.
        limits = c(0, 1),
        breaks = c(0, 0.50, 1),
        minor_breaks = NULL
    ) +
    scale_colour_brewer(         # Use the Color Brewer to choose a palette.
        type = "qual",           # Qualitative set.
        palette = "Paired"       # Name of the palette is 'Paired'.
    ) +
    scale_size_continuous(       # Fine-tune the sizes of each point.
        trans = "exp",           # Use an exp transformation to emphasise loud.
        guide = "none"           # Remove the legend for size.
    ) +
    theme_light() +              # Use a simpler them.
    labs(                        # Make the titles nice.
        x = "Valence",
        y = "Energy",
        colour = "Album"
    )

ggplotly(ve_album)

```

***

In this graph, the Spotify features 'Valence' and 'Energy' are plotted against eachother for all the albums Bring Me The Horizon has made and the loudness of the songs is shown by the size of the dots. As can be seen, their music mostly scores below 0.5 on valence and in general scores really high on energy. An interesting finding in this graph is the shift in valence between the first and last albums. The first album *Count your blessings* scores low on valence, like most of their songs in general. But looking at the last two albums *That's the Spirit* and *amo*, it can be seen that those songs score the highest on valence. Therefore it can be said that the outliers on the valence scale for all Bring Me The Horizon songs, are actually the songs of these last two albums. When looking at the energy scale, there are three main outliers: a song calles 'Memorial' from *There is a Hell...*, 'i apologise if you feel something' from *amo* and 'Fifteen Fantoms, Counting' from *Count your Blessings*. 'Memorial' and 'Fifteen Fantoms, Counting' are both slow instrumental songs and that's the reason they score lower on energy than the rest of the songs. 'i apologise if you feel somthing' is not totally instrumental, but if a slower song compared to the rest of the album. This songs is also used as the intro to Bring Me The Horizon's concert, which is not surprising because it sounds very mysterious. 


### Does 'heavy metal' sound like Heavy Metal?

```{r}
# ceptogram heavy metal 

heavymetal_song <- 
    get_tidy_audio_analysis('6baGTtDakSNvUfW3FJd8yX') %>% 
    compmus_align(bars, segments) %>% 
    select(bars) %>% unnest(bars) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'rms', norm = 'euclidean')) %>% 
    mutate(
        timbre = 
            map(segments, 
                compmus_summarise, timbre, 
                method = 'mean'))

heavymetal_song %>% 
    compmus_gather_timbre %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = basis, 
            fill = value)) + 
    geom_tile() +
    labs(x = 'Time (s)', y = NULL, fill = 'Magnitude') +
    scale_fill_viridis_c(option = 'E') +
    theme_classic()

# ceptogram walk

walk_song <- 
    get_tidy_audio_analysis('7fcfNW0XxTWlwVlftzfDOR') %>% 
    compmus_align(bars, segments) %>% 
    select(bars) %>% unnest(bars) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'rms', norm = 'euclidean')) %>% 
    mutate(
        timbre = 
            map(segments, 
                compmus_summarise, timbre, 
                method = 'mean'))

walk_song %>% 
    compmus_gather_timbre %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = basis, 
            fill = value)) + 
    geom_tile() +
    labs(x = 'Time (s)', y = NULL, fill = 'Magnitude') +
    scale_fill_viridis_c(option = 'E') +
    theme_classic()


```


***

Here you can see two cepstrograms and the differences are immediately noticable. The first cepstrogram is of the song 'heavy metal' from Bring Me The Horizon's latest album *amo*. The second graph is a cepstrogram of the most popular song on Spotify's 'Heavy Metal' playlist 'Walk' by Pantara. Pantera is an American groovemetalband who are known as trendsetters in their genre in the nineties. 

### Does 'heavy metal' have the same timbre structure as Heavy Metal?

```{r}

# heavy metal SSM
heavymetal_song %>% 
    compmus_self_similarity(timbre, 'cosine') %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_classic() +
    labs(x = '', y = '')
    
# walk SSM

walk_song %>% 
    compmus_self_similarity(timbre, 'cosine') %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_classic() +
    labs(x = '', y = '')
    
```

***

Before we looked at the cepstrograms between 'heavy metal' by Bring Me The Horizon and 'walk' by Pantera. These graphs are Self-Similarity Martices based on timbre. Different than the cepstrograms, here we don't see as much differences between the two songs. The horizontal and vertical yellow lines cross about the same time for both songs, this is where in both songs the bridge comes in. In 'heavy metal' the bridge consist of a beatbox section where all the other instruments are dropped. The same happens in 'walk', but instead of a beatbox section, a guitar solo is added with drums. Beatboxing imitates instruments and my guess is that therefore the crossing yellow lines in 'heavy metal' are less clear than in 'walk', where there are just two instruments present in the bridge. 

### Has Bring Me The Horizon changed the way they use keys throughout the years?

```{r}

# codes

circshift <- function(v, n) {if (n == 0) v else c(tail(v, n), head(v, -n))}
                                    
    # C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B 
major_chord <- 
    c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <- 
    c(1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <- 
    c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <- 
    c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
    c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

key_templates <-
    tribble(
        ~name    , ~template,
        'Gb:maj', circshift(major_key,  6),
        'Bb:min', circshift(minor_key, 10),
        'Db:maj', circshift(major_key,  1),
        'F:min' , circshift(minor_key,  5),
        'Ab:maj', circshift(major_key,  8),
        'C:min' , circshift(minor_key,  0),
        'Eb:maj', circshift(major_key,  3),
        'G:min' , circshift(minor_key,  7),
        'Bb:maj', circshift(major_key, 10),
        'D:min' , circshift(minor_key,  2),
        'F:maj' , circshift(major_key,  5),
        'A:min' , circshift(minor_key,  9),
        'C:maj' , circshift(major_key,  0),
        'E:min' , circshift(minor_key,  4),
        'G:maj' , circshift(major_key,  7),
        'B:min' , circshift(minor_key, 11),
        'D:maj' , circshift(major_key,  2),
        'F#:min', circshift(minor_key,  6),
        'A:maj' , circshift(major_key,  9),
        'C#:min', circshift(minor_key,  1),
        'E:maj' , circshift(major_key,  4),
        'G#:min', circshift(minor_key,  8),
        'B:maj' , circshift(major_key, 11),
        'D#:min', circshift(minor_key,  3))

# Chordagram sugar from amo
sugar_chord <- 
    get_tidy_audio_analysis('3Ddgh4ZwVIkLx0f4WeDFmo') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))

sugar_chord  %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '')

# chordagram pray for plagues from Count your Blessings

pray_chord <- 
    get_tidy_audio_analysis('0zYxfgyqypkG7rRwFqyisT') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))

pray_chord  %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '')


    
```

***

On the left two keygrams are shown for two different songs by Bring Me The Horizon. The first is a key visualization of the most popular song from *amo* 'sugar honey ice & tea' and the second one is the most popular song from *Count your Blessings* 'Pray for Plagues'. While the bridge in  'sugar honey ice & tea' comes a bit later than the one in 'Pray for Plagues', they both have one. Both bridges drop a lot of the instruments that can be heard in the rest of the song and slowing it down a bit, making it include less chords at that moment. Also similar in both songs is the usage of the chords. In both songs, the chords that are used the most are used throughout the whole song, so there is no modulation present.

### Which genre is the closest to a song? --> classification 

```{r}

rock <- 
    get_playlist_audio_features('spotify', '37i9dQZF1DWXRqgorJj26U') %>% 
    slice(1:20) %>% 
    add_audio_analysis
punk <- 
    get_playlist_audio_features('spotify', '37i9dQZF1DXasneILDRM7B') %>% 
    slice(1:20) %>% 
    add_audio_analysis
heavymetal <- 
    get_playlist_audio_features('spotify', '37i9dQZF1DX9qNs32fujYe') %>% 
    slice(1:20) %>% 
    add_audio_analysis
pop <- 
    get_playlist_audio_features('spotify', '37i9dQZF1DX1ngEVM0lKrb') %>% 
    slice(1:20) %>% 
    add_audio_analysis
indie <- 
    get_playlist_audio_features('spotify', '37i9dQZF1DX2Nc3B70tvx0') %>% 
    slice(1:20) %>% 
    add_audio_analysis

genre <- 
    rock %>% mutate(playlist = "Rock Classics") %>% 
    bind_rows(
        punk %>% mutate(playlist = "Pure Pop Punk"),
        pop %>% mutate(playlist = "Pop Internacional "),
        indie %>% mutate(playlist = "Ultimate Indie"),
        heavymetal %>% mutate(playlist = "Heavy Metal")) %>% 
    mutate(playlist = factor(playlist)) %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean')) %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(pitches, timbre)

genre_class <- 
    recipe(playlist ~
               danceability +
               energy +
               loudness +
               speechiness +
               acousticness +
               instrumentalness +
               liveness +
               valence +
               tempo +
               duration_ms +
               C + `C#|Db` + D + `D#|Eb` +
               E + `F` + `F#|Gb` + G +
               `G#|Ab` + A + `A#|Bb` + B +
               c01 + c02 + c03 + c04 + c05 + c06 +
               c07 + c08 + c09 + c10 + c11 + c12,
           data = genre) %>% 
    step_center(all_predictors()) %>%
    step_scale(all_predictors()) %>%
    # step_range(all_predictors()) %>% 
    prep(genre) %>% 
    juice

genre_cv <- genre_class %>% vfold_cv(5)

genre_knn <- nearest_neighbor(neighbors = 1) %>% set_engine('kknn')
predict_knn <- function(split)
    fit(genre_knn, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))

predict_knn_reduced <- function(split)
    fit(
        genre_knn, 
        playlist ~ c01 + c11 + liveness + energy + acousticness, 
        data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))
genre_cv %>% 
    mutate(pred = map(splits, predict_knn_reduced)) %>% unnest(pred) %>% 
    metric_set(accuracy, kap, j_index)(truth = playlist, estimate = .pred_class)

genre_cv %>% 
    mutate(pred = map(splits, predict_knn_reduced)) %>% unnest(pred) %>% 
    conf_mat(truth = playlist, estimate = .pred_class) %>% 
    autoplot(type = 'mosaic')

genre %>%
    ggplot(aes(x = c01, y = c11, colour = playlist, size = liveness)) +
    geom_point(alpha = 0.8) +
    scale_color_brewer(type = 'qual', palette = 'Accent') +
    labs(x = 'Timbre Component 1', y = 'Timbre Component 11', size = 'Liveness', colour = 'Playlist')

```



***

I want to use a classifier to find out which genre fits a song the best. If, for example, a song from *Count your Blessings* is closest to the heavy metal genre and a song from *amo* is closest to pop, this will give a good argument in favor of my research question. I have struggled a lot with this weeks homework, because I wasn't able to attend the class. I was able to make the visualizations on the right by using the compmus document, but I wasn't able to make them how I wanted it to and I had trouble reading them. I'm going to work on this for my final portfolio. 

### Clustering

```{r}

# Clustering

bmth_songs <- 
    get_playlist_audio_features('faaske', '6ya8YDyX7ldJsG1OtYWhl4') %>% 
    add_audio_analysis %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean')) %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(pitches, timbre)

# Pre-Processing 

bmth_songs_juice <- 
    recipe(track_name ~
               danceability +
               energy +
               loudness +
               speechiness +
               acousticness +
               instrumentalness +
               liveness +
               valence +
               tempo +
               duration_ms +
               C + `C#|Db` + D + `D#|Eb` +
               E + `F` + `F#|Gb` + G +
               `G#|Ab` + A + `A#|Bb` + B +
               c01 + c02 + c03 + c04 + c05 + c06 +
               c07 + c08 + c09 + c10 + c11 + c12,
           data = bmth_songs) %>% 
    step_center(all_predictors()) %>%
    step_scale(all_predictors()) %>%
    # step_range(all_predictors()) %>% 
    prep(bmth_songs %>% mutate(track_name = str_trunc(track_name, 20))) %>% 
    juice %>% 
    column_to_rownames('track_name')

# Computing distances

bmth_dist <- dist(bmth_songs_juice, method = 'euclidean')

# Hierarchical clustering

protoclust(bmth_dist) %>% dendro_data %>% ggdendrogram


```


*** 

On the first picture we see a clustering of the 69 Bring Me The Horizon songs from all 6 albums. By looking at it we can devide the songs into four sections:
1) From 'Memorial' till 'The Sadness Will Never End'
2) From 'wonderful life' till 'Sleep With One Eye Open'
3) From 'Don't Go' till 'Liquor & Love Lost'
4) From 'ouch' till 'Run' 
The first section can be considered as 'the rest' and so I will focus on the other three sections. 

The 4th section is made up of songs from the last three albums (*Sempiternal*, *That's the Spirit* and *amo*). All the songs from *That's the Spirit* are in this 4th section and the majority of *amo* as well, but for *Sempiternal* this is not the section with the most songs in them. 

*Sempiternal* scores the highest in the 2nd section, with 6 songs from 20 songs in that section. For *Count your Blessings* this section includes the majority of their songs as well (5 songs from the 10 songs on the album). 

When looking at the 3rd section, *There is a Hell Believe Me I've Seen It. There is a Heaven Let's Keep it a Secret* stands out with 8 songs out of 17 in this section. All the other albums except for *That's the Spirit* also have a few songs in this section. 

In general, the most recent albums are clustered together in one section and the older albums are clustered in others. This is very interessting, because it shows that the newer albums are different than the older ones and in such a manner that they (in general) won't cluster with eachother until very high up in the graph. 


### Heatmap

```{r}


# Heatmap

grDevices::dev.size("px")
heatmaply(
    bmth_songs_juice,
    hclustfun = hclust,
    # hclustfun = protoclust,
    # Comment out the hclust_method line when using protoclust.
    hclust_method = 'average',
    dist_method = 'euclidean')

```